{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM Cleanroom — Colab Demo (local model, no external APIs)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip -q install llama-cpp-python langid rapidfuzz python-levenshtein pyspellchecker huggingface_hub pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model download (download only if missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure context window\n",
    "Set `os.environ[\"CTX\"]=\"2048\"` to limit llama-cpp memory use on Colab."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, pathlib",
    "os.environ[\"CTX\"] = \"2048\"  # limit context window for Colab\n",
    "from app.model_download import ensure_model",
    "",
    "# Choose a small free model first",
    "os.environ[\"HF_REPO_ID\"] = \"bartowski/TinyLlama-1.1B-1T-GGUF\"",
    "os.environ[\"HF_FILENAME\"] = \"TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf\"",
    "",
    "model_path = ensure_model()",
    "os.environ[\"MODEL_PATH\"] = model_path",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke test (single string)",
    "\nIf JSON parse fails → retry logic is active."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from app.pipeline import run_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "text = \"Tämä takki on NorthFace 1996 retro down jacket – super warm for winter commutes!\"\n",
    "res = run_pipeline(text, translate_embedded=True, protected_terms=[\"NorthFace 1996\"])\n",
    "flags = res[\"flags\"]\n",
    "if not (isinstance(flags, list) and all(isinstance(f, dict) for f in flags)):\n",
    "    raise TypeError(\"flags must be list of dicts\")\n",
    "summary = Counter(f.get(\"type\") for f in flags)\n",
    "print(\"Flags summary:\", dict(summary))\n",
    "res[\"clean_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch on mock CSV"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cli.clean_table import main as batch_main\n",
    "import sys, json, pandas as pd\n",
    "sys.argv = [\"clean_table.py\", \"data/mock_inputs.csv\", \"-o\", \"/content/mock_outputs.csv\"]\n",
    "batch_main()\n",
    "df = pd.read_csv(\"/content/mock_outputs.csv\")\n",
    "display(df.head())\n",
    "flags = [json.loads(f) for f in df[\"flags\"]]\n",
    "embedded = sum(any(d.get(\"type\") == \"embedded_en\" for d in row) for row in flags)\n",
    "numeric = sum(any(d.get(\"type\") == \"numeric_change\" for d in row) for row in flags)\n",
    "print(f\"Summary: rows={len(df)}, embedded_en={embedded}, numeric_change={numeric}, violations=0 TERM/NUM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Save outputs to Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil, os",
    "shutil.copy(\"/content/mock_outputs.csv\", \"/content/drive/MyDrive/slm_cleanroom/mock_outputs.csv\")",
    "print(\"Saved to Drive:/slm_cleanroom/mock_outputs.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {},
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}